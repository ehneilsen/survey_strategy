# Comparing Survey Strategy Simulations

We have struggled with the best way to analyze and compare simulated surveys. For the report to the SCOC, I propose one way to help would be to define ~25 values to create a report card for each survey simulation. This should make it easier to quickly spot trade-offs, and also if a particular science case is boosted or destroyed. I'm picking 25 since it seems like 5 groups of 5 values is reasonable--more and they become tough to track, fewer and we don't have enough values to cover the needed parameter space.

It would be easy to give the raw value, and then also compared to a fiducial simulation.

A brainstormed list of potential report card values

## SRD and DM values

* An fO value (Probably Nvis median)
* Fraction of observations where a template is available?
* Number of years where ubercal can be run?
* Open Shutter Fraction (or total number of visits, but OSF better if we change exp times)
* coadded depth per filter. Or Sum_filter m-m_fiducial (total depth compared to baseline run). 


## Solar System
Completeness for different populations

* NEOs
* Main Belt
* KBO
* Trojans
* TNO
* Chances of detecting activity in something


## Variables and Transients

Easiest if we ask what fraction of a population get recovered. Maybe do each of these for an isotropic population, and a MW distributed population?

* bright, low amplitude periodic recovery
* faint high amplitude periodic recovery
* Fraction of "fast tranients" (maybe with 80% confident classification pre-peak)
* Fraction of "slow transiets" (that are "well-sampled")

## Galactic Structure

* Median Parallax uncertainty for an m=x star
* Median proper motion uncertainty for an m=x star
* XXX-Something to do with the bulge? Microlensing events?
* XXX-LMC/SMC?

## Cosmology

* number of galaxies that are good for weak lensing
* Fraction of type Ia SNe recovered
* XXX--Deep Drilling fields.  Maybe these get their own section, each DDF gets a grade?

